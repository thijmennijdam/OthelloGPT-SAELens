{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE, LanguageModelSAERunnerConfig, SAEConfig\n",
    "from datasets import load_dataset  \n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_dataset = load_dataset('taufeeque/othellogpt', split='train')\n",
    "# sae = SAE.load_from_pretrained('othello_sae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_tokens = token_dataset[:32][\"tokens\"]\n",
    "# print(batch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae.eval()\n",
    "# sae.cuda()\n",
    "# with torch.no_grad():\n",
    "#     # activation store can give us tokens.\n",
    "#     batch_tokens = token_dataset[:32][\"tokens\"]\n",
    "#     # convert to tensor\n",
    "#     batch_tokens = torch.tensor(batch_tokens)[:, -1]\n",
    "#     _, cache = model.run_with_cache(batch_tokens.cuda(), prepend_bos=False)\n",
    "    \n",
    "    \n",
    "#     feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "#     sae_out = sae.decode(feature_acts)\n",
    "    \n",
    "    \n",
    "#     l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "#     print(\"average l0\", l0.mean().item())\n",
    "#     px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens import utils\n",
    "# from functools import partial\n",
    "\n",
    "# # next we want to do a reconstruction test.\n",
    "# def reconstr_hook(activation, hook, sae_out):\n",
    "#     return sae_out\n",
    "\n",
    "\n",
    "# def zero_abl_hook(activation, hook):\n",
    "#     return torch.zeros_like(activation)\n",
    "\n",
    "\n",
    "# print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "# print(\n",
    "#     \"reconstr\",\n",
    "#     model.run_with_hooks(\n",
    "#         batch_tokens,\n",
    "#         fwd_hooks=[\n",
    "#             (\n",
    "#                 sae.cfg.hook_name,\n",
    "#                 partial(reconstr_hook, sae_out=sae_out),\n",
    "#             )\n",
    "#         ],\n",
    "#         return_type=\"loss\",\n",
    "#     ).item(),\n",
    "# )\n",
    "# print(\n",
    "#     \"Zero\",\n",
    "#     model.run_with_hooks(\n",
    "#         batch_tokens,\n",
    "#         return_type=\"loss\",\n",
    "#         fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "#     ).item(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy tokenizer\n",
    "\n",
    "import string\n",
    "\n",
    "# Initialize the dictionary\n",
    "vocab = {}\n",
    "\n",
    "# Generate the keys (0 to 59) and the corresponding values (A1 to H8)\n",
    "letters = string.ascii_uppercase[:8]  # Get letters 'A' to 'H'\n",
    "index = 0\n",
    "\n",
    "for letter in letters:\n",
    "    for number in range(1, 9):  # Numbers 1 to 8\n",
    "        vocab[index] = f\"{letter}{number}\"\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This happend\n",
      "{'n_layers': 6, 'd_model': 128, 'd_mlp': 512, 'd_head': 64, 'n_heads': 8, 'n_ctx': 59, 'd_vocab': 61, 'act_fn': 'gelu', 'attn_only': False, 'normalization_type': 'LNPre'}\n",
      "Loaded pretrained model my-own-othello-model into HookedTransformer\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "[[20, 21, 34, 19, 13, 40, 47, 28, 12, 41, 35, 5, 10, 43, 3, 23, 6, 26, 42, 54, 52, 51, 59, 36, 39, 44, 11, 49, 4, 27, 32, 38, 33, 1, 25, 60, 48, 17, 50, 57, 29, 58, 55, 14, 22, 46, 30, 24, 9, 31, 16, 8, 56, 7, 45, 18, 15, 2, 37, 53], [34, 42, 27, 19, 50, 51, 52, 35, 41, 48, 49, 56, 36, 58, 11, 60, 20, 40, 57, 30, 24, 28, 59, 2, 21, 44, 22, 12, 13, 26, 4, 14, 15, 23, 5, 43, 33, 32, 10, 8, 16, 47, 55, 54, 46, 45, 37, 7, 53, 31, 39, 9, 29, 38, 25, 18, 3, 6, 17, 1], [20, 21, 22, 13, 14, 19, 5, 15, 29, 11, 16, 8, 28, 6, 27, 33, 4, 3, 18, 7, 2, 12, 41, 10, 32, 23, 9, 37, 24, 40, 39, 50, 25, 17, 59, 47, 1, 42, 51, 36, 54, 55, 35, 30, 34, 43, 52, 26, 48, 60, 56, 46, 45, 57, 38, 58, 31, 49, 44, 53], [34, 28, 21, 12, 23, 42, 13, 22, 33, 39, 41, 14, 43, 50, 29, 27, 40, 49, 4, 24, 16, 20, 57, 48, 19, 30, 18, 51, 32, 52, 5, 26, 47, 9, 60, 37, 31, 35, 17, 38, 7, 6, 58, 3, 45, 56, 2, 59, 44, 8, 36, 15, 11, 25, 46, 10, 55, 53, 1, 54], [20, 19, 41, 21, 11, 28, 10, 40, 39, 49, 34, 38, 14, 13, 29, 27, 48, 3, 2, 50, 46, 15, 4, 35, 59, 23, 32, 37, 24, 43, 25, 51, 6, 1, 52, 30, 36, 12, 16, 56, 5, 22, 18, 8, 55, 9, 57, 42, 26, 44, 47, 53, 45, 7, 33, 31, 17, 60, 54, 58], [20, 19, 34, 42, 18, 12, 4, 28, 29, 10, 35, 5, 1, 30, 50, 3, 43, 49, 48, 26, 41, 17, 44, 56, 57, 21, 23, 15, 24, 16, 36, 33, 8, 22, 51, 39, 11, 60, 6, 40, 59, 7, 25, 58, 55, 54, 46, 53, 13, 31, 32, 27, 9, 52, 14, 2, 37, 38, 45, 47], [34, 28, 23, 41, 48, 49, 22, 47, 57, 29, 20, 14, 21, 50, 30, 11, 59, 15, 46, 35, 42, 43, 19, 40, 36, 52, 8, 44, 51, 56, 3, 27, 18, 39, 32, 54, 60, 33, 5, 45, 53, 55, 37, 38, 26, 6, 7, 10, 17, 2, 13, 4, 25, 58, 9, 12, 1, 24, 16, 31], [27, 33, 39, 21, 42, 41, 40, 38, 26, 46, 34, 51, 13, 25, 37, 47, 32, 48, 20, 14, 43, 50, 60, 52, 17, 9, 22, 35, 44, 36, 49, 12, 3, 4, 57, 31, 30, 45, 7, 2, 53, 29, 24, 28, 19, 23, 6, 15, 5, 55, 8, 11, 56, 18, 10, 59, 58, 16, 1, 54], [20, 33, 42, 21, 19, 34, 13, 50, 41, 48, 38, 10, 39, 32, 18, 45, 2, 5, 51, 52, 43, 1, 58, 3, 46, 11, 37, 54, 40, 25, 36, 47, 12, 27, 26, 31, 29, 35, 28, 23, 30, 49, 60, 59, 15, 7, 14, 16, 56, 57, 17, 55, 4, 9, 44, 22, 53, 6, 8, 24], [20, 19, 41, 40, 39, 42, 27, 49, 10, 38, 34, 22, 51, 11, 57, 28, 12, 2, 33, 13, 3, 48, 14, 15, 29, 35, 46, 30, 37, 1, 55, 58, 21, 26, 56, 45, 9, 18, 23, 5, 8, 53, 16, 43, 59, 4, 36, 60, 44, 54, 6, 50, 17, 52, 32, 24, 25, 47, 7, 31], [20, 21, 34, 41, 14, 27, 49, 7, 22, 35, 32, 50, 51, 11, 6, 5, 19, 26, 42, 33, 2, 23, 13, 57, 40, 28, 39, 37, 24, 12, 56, 55, 31, 10, 4, 47, 38, 30, 9, 3, 45, 48, 18, 46, 36, 15, 54, 60, 43, 44, 29, 25, 8, 16, 52, 17, 58, 59, 1, 53], [27, 19, 11, 21, 20, 10, 34, 41, 9, 3, 2, 17, 48, 39, 22, 57, 4, 29, 33, 23, 1, 18, 47, 46, 14, 6, 43, 32, 42, 28, 25, 35, 38, 13, 31, 44, 54, 45, 24, 30, 40, 56, 49, 26, 7, 15, 36, 50, 53, 8, 37, 12, 55, 16, 52, 51, 59, 60, 58, 5], [34, 28, 20, 42, 23, 33, 43, 22, 51, 11, 2, 50, 39, 24, 35, 19, 14, 47, 46, 6, 41, 27, 18, 9, 55, 4, 32, 38, 15, 56, 17, 7, 21, 25, 59, 58, 3, 52, 49, 48, 29, 57, 37, 13, 5, 10, 1, 36, 45, 26, 31, 12, 44, 54, 53, 40, 30, 16, 8, 60], [27, 33, 40, 47, 42, 19, 54, 51, 28, 29, 60, 34, 20, 21, 32, 37, 22, 43, 18, 23, 52, 17, 15, 35, 9, 14, 24, 49, 5, 38, 45, 46, 50, 53, 30, 13, 56, 1, 4, 6, 41, 55, 11, 58, 59, 44, 26, 25, 57, 2, 31, 7, 16, 10, 3, 8, 12, 48, 36, 39], [27, 21, 28, 33, 40, 18, 13, 48, 38, 32, 56, 49, 31, 20, 26, 46, 39, 23, 41, 6, 16, 14, 57, 50, 43, 59, 51, 55, 53, 58, 12, 34, 42, 35, 52, 11, 10, 9, 19, 25, 47, 22, 7, 54, 37, 44, 36, 15, 1, 8, 2, 30, 5, 4, 24, 29, 17, 45, 60, 3], [27, 21, 22, 33, 41, 49, 39, 40, 50, 38, 28, 20, 48, 57, 11, 29, 34, 23, 47, 43, 37, 58, 14, 56, 55, 15, 51, 31, 45, 46, 54, 19, 26, 6, 59, 2, 25, 35, 30, 32, 10, 53, 12, 24, 18, 3, 52, 60, 13, 4, 16, 44, 42, 8, 36, 17, 7, 5, 9, 1], [27, 19, 20, 33, 10, 21, 39, 26, 22, 47, 34, 42, 31, 11, 12, 25, 50, 51, 46, 49, 41, 5, 4, 13, 28, 2, 3, 14, 17, 53, 35, 9, 15, 8, 23, 38, 7, 18, 55, 44, 29, 30, 57, 56, 16, 24, 36, 58, 60, 43, 1, 40, 59, 32, 48, 52, 45, 6, 37, 54], [20, 33, 42, 12, 27, 18, 4, 34, 26, 50, 9, 21, 39, 46, 41, 40, 48, 55, 14, 47, 43, 17, 59, 29, 38, 10, 24, 22, 54, 53, 13, 49, 57, 37, 25, 28, 2, 35, 44, 51, 52, 36, 30, 58, 56, 7, 19, 60, 31, 5, 15, 8, 45, 32, 6, 3, 23, 16, 11, 1], [27, 33, 42, 19, 10, 20, 11, 51, 40, 28, 39, 12, 5, 4, 60, 6, 34, 9, 2, 26, 17, 48, 29, 25, 47, 18, 57, 21, 14, 55, 46, 35, 38, 23, 43, 31, 32, 52, 16, 30, 22, 41, 36, 56, 15, 24, 50, 45, 37, 58, 13, 7, 1, 44, 49, 3, 59, 8, 54, 53], [34, 28, 21, 40, 35, 23, 27, 18, 26, 12, 42, 32, 39, 41, 22, 36, 13, 48, 44, 43, 51, 52, 47, 46, 53, 33, 54, 60, 59, 29, 24, 14, 3, 49, 6, 30, 19, 16, 56, 58, 9, 38, 5, 57, 15, 11, 7, 8, 31, 50, 10, 37, 20, 25, 17, 55, 45, 4, 1, 2], [27, 33, 41, 34, 38, 48, 49, 18, 19, 20, 21, 50, 11, 4, 43, 32, 31, 52, 22, 37, 9, 46, 12, 25, 17, 14, 13, 5, 35, 10, 57, 40, 36, 15, 28, 23, 7, 29, 24, 3, 26, 6, 54, 45, 42, 44, 53, 47, 1, 30, 55, 2, 16, 59, 58, 51, 60, 8, 39, 56], [20, 33, 41, 12, 27, 40, 19, 50, 21, 13, 5, 32, 28, 4, 3, 18, 10, 29, 48, 1, 38, 46, 22, 55, 9, 14, 59, 11, 6, 26, 36, 30, 35, 23, 39, 44, 17, 49, 37, 51, 53, 25, 42, 45, 43, 52, 57, 54, 24, 2, 31, 15, 7, 34, 47, 56, 60, 16, 8, 58], [20, 21, 42, 27, 18, 19, 11, 39, 28, 34, 22, 29, 30, 9, 46, 50, 40, 13, 35, 43, 51, 2, 4, 33, 49, 47, 32, 52, 3, 24, 54, 5, 36, 48, 56, 57, 14, 26, 58, 7, 44, 12, 15, 38, 31, 60, 41, 10, 17, 37, 45, 53, 16, 6, 1, 59, 8, 55, 23, 25], [34, 42, 41, 28, 43, 52, 44, 50, 29, 40, 58, 24, 27, 33, 60, 49, 26, 32, 39, 20, 30, 38, 11, 18, 46, 22, 19, 53, 9, 2, 13, 51, 21, 59, 23, 15, 31, 57, 16, 12, 47, 55, 3, 45, 35, 4, 6, 25, 14, 5, 48, 37, 56, 8, 1, 7, 10, 36, 54, 17], [20, 19, 27, 33, 10, 22, 34, 11, 40, 13, 15, 1, 28, 43, 26, 42, 35, 48, 39, 23, 49, 32, 21, 50, 52, 17, 2, 3, 57, 7, 38, 46, 37, 31, 25, 14, 54, 53, 12, 36, 24, 30, 5, 55, 56, 47, 29, 44, 4, 41, 8, 60, 45, 59, 58, 6, 16, 18, 9, 51], [41, 28, 19, 39, 29, 50, 46, 27, 33, 21, 22, 32, 25, 10, 13, 23, 15, 47, 38, 5, 1, 8, 55, 30, 35, 18, 7, 42, 4, 53, 51, 40, 17, 24, 16, 48, 56, 57, 34, 36, 45, 49, 58, 3, 44, 43, 26, 37, 20, 31, 52, 54, 6, 11, 9, 12, 2, 14, 60, 59], [20, 21, 42, 39, 34, 19, 33, 43, 12, 5, 46, 50, 22, 32, 27, 11, 35, 36, 4, 48, 10, 3, 41, 14, 28, 23, 15, 7, 52, 1, 58, 40, 26, 57, 9, 17, 6, 49, 25, 2, 55, 18, 24, 59, 13, 47, 29, 45, 44, 60, 38, 37, 54, 30, 16, 56, 8, 53, 31, 51], [34, 42, 20, 33, 32, 19, 18, 10, 41, 17, 1, 28, 29, 2, 51, 60, 43, 40, 3, 50, 26, 38, 47, 56, 46, 23, 15, 44, 21, 22, 52, 16, 58, 30, 11, 27, 49, 9, 35, 54, 31, 7, 14, 37, 39, 48, 59, 4, 53, 25, 24, 5, 55, 13, 57, 36, 6, 8, 12, 45], [27, 21, 14, 26, 20, 33, 42, 34, 41, 11, 40, 7, 32, 48, 47, 50, 13, 22, 28, 46, 43, 23, 18, 9, 58, 4, 2, 49, 57, 29, 19, 59, 24, 37, 54, 39, 60, 17, 38, 30, 6, 15, 35, 44, 25, 53, 31, 3, 12, 16, 8, 56, 55, 1, 5, 36, 52, 10, 45, 51], [41, 28, 22, 40, 27, 26, 39, 33, 17, 50, 20, 32, 37, 21, 59, 48, 56, 46, 29, 57, 13, 18, 34, 30, 23, 11, 55, 42, 9, 53, 36, 38, 47, 24, 15, 25, 12, 14, 49, 1, 51, 52, 10, 2, 54, 5, 31, 45, 58, 44, 43, 3, 19, 16, 4, 60, 8, 35, 7, 6], [20, 21, 28, 33, 13, 14, 22, 29, 41, 42, 6, 7, 34, 12, 8, 23, 24, 36, 50, 35, 5, 58, 38, 32, 43, 48, 49, 45, 55, 27, 30, 4, 26, 17, 11, 16, 39, 47, 44, 18, 10, 19, 25, 2, 15, 40, 59, 60, 1, 51, 37, 3, 56, 57, 31, 46, 53, 52, 9, 54], [27, 19, 20, 33, 18, 12, 41, 40, 13, 26, 32, 31, 4, 28, 11, 39, 47, 5, 17, 21, 14, 46, 34, 22, 45, 3, 2, 25, 10, 7, 35, 42, 51, 6, 49, 1, 23, 60, 37, 55, 29, 16, 50, 48, 38, 53, 58, 59, 24, 30, 15, 43, 54, 9, 36, 56, 8, 44, 52, 57]]\n",
      "torch.Size([32, 59])\n",
      "Creating SaeVisData\n",
      "Creating SaeVisData\n",
      "Getting feature data\n",
      "objects created\n",
      "feature_batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis: 100%|██████████| 1/1 [00:06<00:00,  6.76s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time  </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.02s │ 0.3%  │\n",
       "│ (2) Forward passes to gather model activations │ 6.71s │ 87.1% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.02s │ 0.3%  │\n",
       "│ (4) Getting data for tables                    │ 0.06s │ 0.7%  │\n",
       "│ (5) Getting data for histograms                │ 0.09s │ 1.1%  │\n",
       "│ (6) Getting data for sequences                 │ 0.76s │ 9.9%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.05s │ 0.7%  │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.02s │ 0.3%  │\n",
       "│ (2) Forward passes to gather model activations │ 6.71s │ 87.1% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.02s │ 0.3%  │\n",
       "│ (4) Getting data for tables                    │ 0.06s │ 0.7%  │\n",
       "│ (5) Getting data for histograms                │ 0.09s │ 1.1%  │\n",
       "│ (6) Getting data for sequences                 │ 0.76s │ 9.9%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.05s │ 0.7%  │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis: 100%|██████████| 1/1 [00:07<00:00,  7.73s/it]\n",
      "Extracting vis data from cached data: 100%|██████████| 11/11 [00:07<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "import sae_vis\n",
    "import importlib\n",
    "importlib.reload(sae_vis)\n",
    "importlib.reload(sae_vis.data_storing_fns)\n",
    "from sae_vis import utils_fns\n",
    "importlib.reload(utils_fns)\n",
    "# importlib.reload(sae_vis.data_config_classes)\n",
    "from sae_vis import data_fetching_fns\n",
    "importlib.reload(data_fetching_fns)\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "sae = SAE.load_from_pretrained('othello_sae')\n",
    "\n",
    "\n",
    "test_feature_idx_gpt = list(range(10)) + [10]\n",
    "hook_name = sae.cfg.hook_name\n",
    "model = HookedTransformer.from_pretrained('my-own-othello-model')\n",
    "model.work_around_vocab = vocab\n",
    "\n",
    "# batch_tokens = token_dataset[:32][\"tokens\"]\n",
    "# convert to tensor\n",
    "# batch_tokens = torch.tensor(batch_tokens)[:, -1]\n",
    "    \n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_name,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=10,\n",
    "    minibatch_size_tokens=59,\n",
    "    verbose=True,\n",
    ")\n",
    "token_dataset = load_dataset('taufeeque/othellogpt', split='train')\n",
    "print(type(token_dataset))\n",
    "print(token_dataset[:32][\"tokens\"])\n",
    "# convert to tensor\n",
    "token_dataset = torch.tensor(token_dataset[:32][\"tokens\"])[:, :-1].cuda()\n",
    "print(token_dataset.shape)\n",
    "# print(token_dataset[:32][\"tokens\"])\n",
    "sae.eval()\n",
    "\n",
    "sae_vis_data_gpt = SaeVisData.create(\n",
    "    encoder=sae.cuda(),\n",
    "    model=model, # type: ignore\n",
    "    tokens=token_dataset,  # type: ignore\n",
    "    cfg=feature_vis_config_gpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "import webbrowser\n",
    "import http.server\n",
    "import platform\n",
    "\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "from IPython.display import IFrame, display\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    if not COLAB:\n",
    "        if os.path.isfile(filename):\n",
    "            print(f\"Opening {filename}\")\n",
    "            # Print current folder\n",
    "            print(f\"Current folder: {os.getcwd()}\")\n",
    "            # Get the absolute path\n",
    "            file_path = os.path.abspath(filename)\n",
    "    \n",
    "            # Check if running under WSL\n",
    "            if 'microsoft-standard' in platform.uname().release:\n",
    "                # Use the Windows default browser to open the file\n",
    "                windows_path = subprocess.check_output(['wslpath', '-w', file_path]).decode().strip()\n",
    "                subprocess.run(['explorer.exe', windows_path])\n",
    "            else:\n",
    "                # For other environments, use the default web browser\n",
    "                webbrowser.open(f\"file://{file_path}\")\n",
    "            \n",
    "            # Display inline in Jupyter Notebook\n",
    "            display(IFrame(src=f\"file://{file_path}\", width=\"100%\", height=height))\n",
    "        else:\n",
    "            print(f\"File not found: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 0_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/0_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc94d4771d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 1_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/1_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcadcdf2c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 2_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/2_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcadcdf2c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 3_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/3_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcadcdf2c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 4_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/4_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8cac10790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 5_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/5_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8cb247110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 6_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/6_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8cb31d110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 7_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/7_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8caeece90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 8_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/8_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8cb501490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 9_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/9_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8cb122550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 11/11 [00:01<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 10_feature_vis_demo_gpt.html\n",
      "Current folder: /mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"850\"\n",
       "            src=\"file:///mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/10_feature_vis_demo_gpt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8caf721d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for feature in test_feature_idx_gpt:\n",
    "    filename = f\"{feature}_feature_vis_demo_gpt.html\"\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    display_vis_inline(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
